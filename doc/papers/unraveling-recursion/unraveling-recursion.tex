\input{preamble-lncs}

\title{Unraveling recursion: compiling an IR with recursion to System F}

\author{
Michael Peyton Jones\inst{1}\orcidID{0000-0003-0602-1657}
\and Vasilis Gkoumas\inst{1}
\and Roman Kireev\inst{1}\orcidID{0000-0003-4687-2739}
\and Kenneth MacKenzie\inst{1}
\and Chad Nester\inst{2}
\and Philip Wadler\inst{2}\orcidID{0000-0001-7619-6378}
}

\institute{IOHK\\
\email{\{roman.kireev,michael.peyton-jones,vasilis.gkoumas,kenneth.mackenzie\}@iohk.io}
\and University of Edinburgh\\
\email{\{cnester,wadler\}@inf.ed.ac.uk}
}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
% Add stuff here as latex complains about it
\DeclareUnicodeCharacter{2080}{\ensuremath{{}_0}}
\DeclareUnicodeCharacter{2081}{\ensuremath{{}_1}}
\DeclareUnicodeCharacter{2082}{\ensuremath{{}_2}}
\DeclareUnicodeCharacter{22A4}{\ensuremath{\top}}
\DeclareUnicodeCharacter{228E}{\ensuremath{\uplus}}
\DeclareUnicodeCharacter{1D57}{\ensuremath{{}^t}}
\DeclareUnicodeCharacter{1D49}{\ensuremath{{}^e}}
\DeclareUnicodeCharacter{2200}{\ensuremath{\forall}}
\DeclareUnicodeCharacter{2218}{\ensuremath{\circ}}
\DeclareUnicodeCharacter{3BB}{\ensuremath{\lambda}}
\DeclareUnicodeCharacter{2217}{\ensuremath{\ast}}
\DeclareUnicodeCharacter{2192}{\ensuremath{\rightarrow}}
\DeclareUnicodeCharacter{00D7}{\ensuremath{\times}}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{verbatim}

\usepackage{etoolbox}

\usepackage{todonotes}

\usepackage{semantic}

\usepackage{mathpartir}

\usepackage{hyperref}
\usepackage[capitalise, noabbrev]{cleveref}

% If true, we try and include the agda style file and generated latex from
% the 'latex' dir. If false we just include the original lagda files and
% redefine 'code' to be a verbatim environment, which is nice for development.
\newtoggle{lagda}
\togglefalse{lagda}
% This is a bit of a hack to make it easy to toggle on including
% the generated literate Agda latex if it's been built:
% - Run agda --latex over all the files
% - echo '\toggletrue{lagda}' > agdaswitch.tex
% - Built the latex
\InputIfFileExists{agdaswitch}{}{}

\iftoggle{lagda}{
  % agda.sty wants to use the deprecated utf8x option, which
  % many publishers don't like. So we do it ourselves
  \usepackage{agda}
}{
  \newenvironment{code}
      {\verbatim}
      {\endverbatim}
}

\newcommand{\inputlagda}[1]{\iftoggle{lagda}{\input{#1.tex}}{\input{#1.lagda}}}

\newcommand{\sourceUrl}{https://github.com/input-output-hk/plutus/tree/3008f78ed7f75cdd98da7fb06f06345fc52c2e31/papers/unraveling-recursion}

% TODO: non-awful names
\newcommand{\SF}{\ensuremath{\textrm{System}\ F}}
\newcommand{\FOM}{\ensuremath{\textrm{System}\ F_\omega}}
\newcommand{\FOMF}{\ensuremath{\textrm{System}\ F_{\omega}^\mu}}
\newcommand{\FIR}{\ensuremath{\textrm{FIR}}}

% Not Steele's recommendation, but we're not doing anything that could
% be confused with state updating in this paper.
\newcommand{\subst}[2]{[#1 := #2]}
\newcommand{\substIn}[3]{#3\subst{#1}{#2}}

% fixpoints
\DeclareMathOperator{\fixo}{\texttt{fix}}
% metalanguage version for functions
\DeclareMathOperator{\fixml}{\mathsf{fix}}
\DeclareMathOperator{\Fix0}{\mathsf{Fix}_0}
\DeclareMathOperator{\fix2}{\mathsf{fix}_2}
\DeclareMathOperator{\fixBy}{\mathsf{fixBy}}

% fir syntax bits
\newcommand{\typeArrow}{\rightarrow}
\newcommand{\kindArrow}{\Rightarrow}
\DeclareMathOperator{\ifix}{\texttt{ifix}}
\DeclareMathOperator{\wrap}{\texttt{wrap}}
\DeclareMathOperator{\unwrap}{\texttt{unwrap}}
\DeclareMathOperator{\Type}{\ast}
\DeclareMathOperator{\tlet}{\texttt{let}}
\DeclareMathOperator{\tin}{\texttt{in}}
\DeclareMathOperator{\rec}{\texttt{rec}}
\DeclareMathOperator{\with}{\texttt{with}}

% tuples
\DeclareMathOperator{\curry}{\textsf{curry}}
\DeclareMathOperator{\uncurry}{\textsf{uncurry}}

% maybe
\DeclareMathOperator{\Maybe}{\textsf{Maybe}}
\DeclareMathOperator{\Just}{\textsf{Just}}
\DeclareMathOperator{\Nothing}{\textsf{Nothing}}
\DeclareMathOperator{\Match}{\textsf{match}}

% tree forest
\DeclareMathOperator{\Tree}{\textsf{Tree}}
\DeclareMathOperator{\Forest}{\textsf{Forest}}
\DeclareMathOperator{\Node}{\textsf{Node}}

% misc
\DeclareMathOperator{\Int}{\textsf{Int}}
\DeclareMathOperator{\Bool}{\textsf{Bool}}
\DeclareMathOperator{\FFalse}{\textsf{False}}
\DeclareMathOperator{\TTrue}{\textsf{True}}
\DeclareMathOperator{\NNat}{\textsf{Nat}}
\DeclareMathOperator{\ZZero}{\textsf{Z}}
\DeclareMathOperator{\SSuc}{\textsf{S}}
\DeclareMathOperator{\List}{\textsf{List}}
% avoid clashing with agda.sty
\DeclareMathOperator{\NNil}{\textsf{Nil}}
\DeclareMathOperator{\CCons}{\textsf{Cons}}
\DeclareMathOperator{\Unit}{\textsf{Unit}}
\DeclareMathOperator{\Unitval}{\textsf{()}}
\DeclareMathOperator{\by}{\textsf{by}}
\DeclareMathOperator{\sel}{\textsf{sel}}

\newcommand{\compileterm}{\mathbb{C}_{\mathrm{term}}}
\newcommand{\compiletermrec}{\mathbb{C}_{\mathrm{termrec}}}
\newcommand{\compiletype}{\mathbb{C}_{\mathrm{type}}}
\newcommand{\compiledata}{\mathbb{C}_{\mathrm{data}}}
\newcommand{\compiledatarec}{\mathbb{C}_{\mathrm{datarec}}}

\newcommand{\seq}[1]{\overline{#1}}
\newcommand{\fixed}[1]{\underline{#1}}
\newcommand{\length}[1]{\lvert #1 \rvert}
\newcommand{\defeq}{:=}
\newcommand{\seqFunArr}[2]{\overline{#1 \rightarrow}\ #2}
\newcommand{\seqKindArr}[2]{\overline{#1 \Rightarrow}\ #2}

\newcommand{\todompj}[1]{\todo[inline,color=yellow!40,author=Michael]{#1}}
\newcommand{\todovg}[1]{\todo[inline,color=blue!40,author=Vasilis]{#1}}
\newcommand{\todork}[1]{\todo[inline,color=orange!40,author=Roman]{#1}}

\newcommand{\fomfDiff}[1]{\text{\colorbox{lightgray}{\(#1\)}}}
\newcommand{\firDiff}[1]{\text{\colorbox{yellow}{\(#1\)}}}

\DeclareMathOperator{\Data}{\texttt{data}}
\newcommand{\datatype}[4]{\Data{}\, #1 ~ #2 = #4 \, \with\, #3}

\newcommand{\branchTy}[2]{\mathsf{branchTy}(#1, #2)}
\newcommand{\dataDecl}[1]{\mathsf{dataDec}(#1)}

\newcommand{\dataKind}[1]{\mathsf{dataKind}(#1)}
\newcommand{\scottTy}[1]{\mathsf{dataTy}(#1)}
\newcommand{\constr}[3]{\mathsf{constr}_{#3}(#1, #2)}
\newcommand{\constrs}[1]{\mathsf{constrs}(#1)}
\newcommand{\constrTy}[2]{\mathsf{constrTy}(#1, #2)}
\newcommand{\match}[1]{\mathsf{match}(#1)}
\newcommand{\matchTy}[1]{\mathsf{matchTy}(#1)}
\newcommand{\unveil}[2]{\mathsf{unveil}(#1, #2)}

\newcommand{\dataBind}[1]{\mathsf{dataBind}(#1)}
\newcommand{\matchBind}[1]{\mathsf{matchBind}(#1)}
\newcommand{\constrBind}[2]{\mathsf{constrBind}(#1, #2)}
\newcommand{\constrBinds}[1]{\mathsf{constrBinds}(#1)}
\newcommand{\binds}[1]{\mathsf{binds}(#1)}

\newcommand{\tagKind}[1]{\mathsf{tagKind}(#1)}
\newcommand{\dtTag}[3]{\mathsf{tag}_{#3}(#1, #2)}
\newcommand{\dtInst}[4]{\mathsf{inst}_{#4}(#1, #2, #3)}
\newcommand{\dtFamily}[1]{\mathsf{family}(#1)}
\newcommand{\dtInstFinal}[3]{\mathsf{instFamily}_{#3}(#1, #2)}

\newcommand{\constrRec}[5]{\mathsf{constr}^{\rec}_{#4, #5}(#1, #2, #3)}
\newcommand{\constrsRec}[3]{\mathsf{constrs}^{\rec}_{#3}(#1, #2)}
\newcommand{\matchRec}[3]{\mathsf{match}^{\rec}_{#3}(#1, #2)}
\newcommand{\unveilRec}[2]{\mathsf{unveil}^{\rec}(#1, #2)}

\begin{document}

\maketitle

\begin{abstract}
Lambda calculi are often used as intermediate representations for
compilers. However, they require extensions to handle higher-level
features of programming languages. In this paper we show how to
construct an IR based on \FOMF{} which supports recursive functions
and datatypes, and describe how to compile it to \FOMF{}.  Our IR was
developed for commercial use at the IOHK company, where it is used as
part of a compilation pipeline for smart contracts running on a
blockchain.
\end{abstract}

\section{Introduction}

Many compilers make use of \emph{intermediate representations} (IRs) as stepping
stones between their source language and their eventual target language. Lambda
calculi are tempting choices as IRs for functional programming languages. They
are simple, well-studied, and easy to analyze.

However, lambda calculi also have several features that make them poor IRs.
\begin{itemize}
  \item They are hard to read and write. Although they are mostly read and
    written by computers, this complicates writing compilers and
    debugging their output.
  \item They can be hard to optimize. Some optimizations are much easier to
    write on a higher-level language. For example, dead-binding elimination is
    much easier with explicit let-bindings.
  \item They make the initial compilation step ``too big''. Compiling all the way
    from a high-level surface language to a lambda calculus can involve many
    complex transformations, and it is often advantageous from an engineering
    standpoint to break it into smaller steps.
\end{itemize}

Hence it is common to design an IR by extending a lambda calculus with
additional features which make the IR more legible, easier to optimize, or
closer to the source language (e.g. GHC Core \cite{jones1998transformation}, Henk \cite{Henk}, Idris' TT
\cite{brady2013idris}, and OCaml's Lambda \cite{leroy1990zinc}). However, given that such IRs are desirable,
there is little material on implementing or compiling them.

In this paper we construct an IR
suitable for a powerful functional programming language like Haskell.
We take as our lambda calculus \FOMF{} (\FOM{} with indexed
fixpoints: see \cite[Chapter 30]{pierce2002types}, formalized recently in \cite{chapman2019systemf}), which allows us to talk about higher-kinded recursive types,
and extend it to an IR called \FIR{} which adds the following features:
\begin{itemize}
  \item Let-binding of non-recursive terms, types, and datatypes.
  \item Let-binding of recursive terms and datatypes.
\end{itemize}

This is a small, but common, subset of the higher-level features that
functional programming languages usually have, so this provides a reusable IR
for compiler writers targeting \FOMF{}.

Moreover, all of the compilation passes that we provide are \emph{local} in the
sense that they do not access more than one level of the syntax tree, and they
do not require any type information that is not present in type annotations. So
while we provide typing rules for \FIR{}, it is not necessary to perform type
synthesis in order to compile it.

Encoding recursive terms has traditionally been done with fixpoint combinators.
However, the textbook accounts typically do not cover mutual recursion, and
where it \emph{is} handled it is often assumed that the calculus is non-strict. We
construct a generalized, polyvariadic fixpoint combinator that works in both strict
and non-strict base calculi, which we use to compile recursive terms.

In order to compile datatypes, we need to encode them and their
accompanying constructors and destructors using the limited
set of types and terms we have available in our base calculus.
The Church encoding (\cite[Chapter 5.2, Chapter 23.4]{pierce2002types})
is a well-known method of doing this in plain \SF{}.
With it, we can encode even recursive datatypes, so long as the
recursion occurs only in positive positions.

However, some aspects of the Church encoding are not ideal, for example, it requires time
proportional to the size of a list to extract its tail.
We use a different encoding, the Scott encoding \cite{plotkin}, which can encode any
recursive datatype, but requires adding a fixpoint operator to \SF{} in order to
handle arbitrary type-level recursion.

To handle mutually recursive datatypes we borrow
some techniques from the generic programming community, in particular indexed
fixpoints, and the use of type-level tags to combine a family of mutually
recursive datatypes into a single recursive datatype. While this technique is
well-known (see e.g. \cite{fixmutualgeneric}), the details of our approach
are different, and we face some additional constraints because we are targeting \FOMF{}
rather than a full dependently-typed calculus.

We have used \FIR{} as an IR in developing Plutus \cite{plutusgithub}, a
platform for developing smart contracts targeting the Cardano blockchain. Users
write programs in Haskell, which are compiled by a GHC compiler plugin into
Plutus Core, a small functional programming language. Plutus Core is an extension
of \FOMF{}, so in order to easily compile Haskell's high-level language features we
developed \FIR{} as an IR above Plutus Core. We have used this compiler to write
substantial programs in Haskell and compile them to Plutus Core, showing that
the techniques in this paper are usable in practice. The compiler is available
for public use at \cite{plutusplayground}.

\paragraph{Contributions.}

We make the following contributions.

\begin{itemize}
  \item We give syntax and typing rules for \FIR{}, a typed IR
    extending \FOMF{}.
  \item We define a series of local compilation passes which
    collectively compile \FIR{} into \FOMF{}.
  \item We provide a reference implementation of the syntax, type system, and
    several of the compilation passes in Agda \cite{norell2007towards}, a powerful dependently
    typed programming language.
  \item We have written a complete compiler implementation in Haskell as part of
    a production system for the Plutus platform.
\end{itemize}

Our techniques for encoding datatypes are not novel
\cite{fixmutualgeneric}\cite{loh2011generic}. However, we
know of no complete presentation that handles mutual recursion and
parameterized datatypes, and targets a calculus as small as \FOMF{}.

We believe our techniques for encoding mutually recursive functions are novel.

While the Agda compiler implementation is incomplete, and does not include soundness
proofs, we believe that the very difficulty of doing this makes our partial
implementation valuable. We discuss the difficulties further in \cref{sec:compiler}.

\paragraph{Note on the use of Agda.}

Although \FOMF{} is a complete programming language in its own
right, it is are somewhat verbose and clumsy to use for the \emph{exposition} of the
techniques we are presenting.

Consequently we will use:
\begin{itemize}
  \item Agda code, typeset colourfully, for exposition.
  \item \FOMF{} code, typeset plainly, for the formal descriptions.
\end{itemize}

We have chosen to use $\Type$ for the kind of types, whereas Agda normally
uses Set. To avoid confusion we have aliased Set to $\Type$ in our Agda code.
Readers should recall that Agda uses $\rightarrow$ following binders rather than
a $.$ character.

The Agda code in this paper and the Agda compiler code are available in
\href{\sourceUrl}{the Plutus repository}.

\paragraph{Notational conventions.}

We will omit kind signatures in \FOMF{} when they are $\Type$, and any other signatures when
they are obvious from context or repetition.

We will be working with a number of constructs that have sequences of elements. We
will adopt the metalanguage conventions suggested by Guy Steele \cite{steele2017s}, in
particular:
\begin{itemize}
\item $\substIn{x}{v}{t}$ is a substitution of $v$ for $x$ in $t$.
\item $\seq{t}$ is expanded to any number of (optionally separated) copies of $t$.
  Any underlined portions of $t$ must be expanded the same way in each copy. Where we require
  access to the index, the overline is superscripted with the index. For example:
  \begin{itemize}
    \item $\seq{x : T}$ is expanded to $x_1 : T_1 \dots x_n : T_n$
    \item $\seq{\fixed{\Gamma} |- J}$ is expanded to $\Gamma |- J_1 \dots \Gamma : J_n$
    \item $\seq{x_j : T_{j+1}}^j$ is expanded to $x_1 : T_2 \dots x_n : T_{n+1}$
  \end{itemize}
\item $\seqFunArr{t}{u}$ is expanded to $t_1 \rightarrow ~\dots ~ \rightarrow
  t_n \rightarrow u$, similarly for $\kindArrow$.
\end{itemize}

\section{Datatype encodings}
\label{sec:data-encoding}

The Scott encoding represents a datatype as the type of the pattern-matching
functions on it. For example, the type of booleans, $\Bool$, is encoded as
\begin{displaymath}
  \forall R . R \rightarrow R \rightarrow R
\end{displaymath}
That is, for any output type $R$ you like, if you provide an $R$ for
the case where the value is false and an $R$ for the case where the value is
true, then you have given a method to construct an $R$ from all possible booleans,
thus performing a sort of pattern-matching. In
general the arguments to the encoded datatype value are functions which
transform the arguments of each constructor into an $R$.

The type of naturals, $\NNat$, is encoded as
\begin{displaymath}
  \forall R . R \rightarrow (\NNat \rightarrow R) \rightarrow R
\end{displaymath}
Here we see an occurrence of $\NNat$ in the definition, which
corresponds to recursive use in the ``successor'' constructor.
We will need type-level recursion to deal with recursive references.

The Church encoding of $\Bool$ is the same as the Scott encoding. This is true
for all non-recursive datatypes, but not for recursive datatypes. The Church
encoding of $\NNat$ is:
\begin{displaymath}
  \forall R . R \rightarrow (R \rightarrow R) \rightarrow R
\end{displaymath}
Here the recursive occurrence of $\NNat$ has disappeared, replaced by
an $R$. This is because while the Scott encoding corresponds to a pattern-match on a type,
the Church encoding corresponds to a \emph{fold}, so recursive occurrences have
already been folded into the output type.

This highlights the tradeoffs between the two encodings (see \cite{scott} for
further discussion):
\begin{itemize}
  \item To operate on a Church encoded value we must perform a fold on the
    entire structure, which is frequently inefficient. For a Scott encoded value,
    we only have to inspect the surface level of the term, which is inexpensive.
  \item Since recursive occurrences of the type are already ``folded'' in the
    Church encoding, there is no need for a type-level recursion operator. Contrast
    this with the situation with the Scott encoding, in which additional type-level machinery
    (fixed points) is needed to define type-level recursion.
\end{itemize}

In this paper we will use the Scott encoding to encode datatypes.

\section{Syntax and type system of \FOMF{} and \FIR{}}

\FIR{} is an extension of \FOMF{}, which is itself an extension of the
well-known \FOM{}. In the following figures we give
\begin{itemize}
\item Syntax (\cref{fig:fir_syntax})
\item Kinding (\cref{fig:fir_kinding})
\item Well-formedness of constructors and bindings (\cref{fig:fir_wellformed})
\item Type equivalence (\cref{fig:fir_typeq})
\item Type synthesis (\cref{fig:fir_typing})
\end{itemize}
for full \FIR{}. Cases without highlighting are for
\FOM{}, while we highlight additions for \fomfDiff{\FOMF{}} and \firDiff{\FIR{}}.

There are a number of auxiliary definitions in \cref{fig:fir_aux} for dealing
with datatypes and bindings. These define kinds and types for the various
bindings produced by datatype bindings. We will go through examples of how they
work in \cref{sec:non-recursive-data}.

\input{grammar.tex}
\input{kinding.tex}
\input{typing.tex}

\input{Fixpoints}

\subsection{Datatypes}

\FIR{} includes \emph{datatypes}. A \FIR{} datatype defines a type
with a kind, parameterized by several type variables. The right-hand side
declares of a list of constructors with type arguments, and the name
of a matching function.\footnote{Why declare a matching function explicitly,
  rather than using case expressions? The answer is that we want to be
  \emph{local}: matching functions can be defined and put into scope when
  processing the datatype binding, whereas case expressions require additional
  program analysis to mach up the expression with the corresponding datatype.}
They thus are similar to the familiar style of defining datatypes in languages such as Haskell.

For example,
\begin{displaymath}
\datatype{\Maybe}{(A :: \Type)}{\textsf{matchMaybe}}{(\Nothing (), \Just (A))}
\end{displaymath}
defines the familiar $\Maybe$ datatype, with constructors $\Nothing$ and
$\Just$, and matching function $\texttt{matchMaybe}$.

The type of $\texttt{matchMaybe}$ is $\Maybe A \rightarrow \forall R . R
\rightarrow (A \rightarrow R) \rightarrow R$. This acts as a pattern-matching
function on $\Maybe$ \textemdash{} exactly as we saw the Scott encoding behave
in \cref{sec:data-encoding}. The matcher converts the
abstract datatype into the raw, Scott-encoded type which can be used as a
pattern matcher. We will see the full details in
\cref{sec:non-recursive-data}, and the type is given by $\matchTy{\Maybe}$ as
defined in \cref{fig:compile-datatypes}.

Since \FIR{} includes recursive datatypes, we could have removed $\ifix$,
$\wrap$ and $\unwrap$ from \FIR{}. However, in practice it is useful for the
target language (\FOMF{}) to be a true subset of the source language (\FIR{}),
as this allows us to implement compilation as a series of \FIR{}-to-\FIR{} passes.

\subsection{Let}

\FIR{} also features let terms. These have a number of
bindings in them which bind additional names which are in scope inside the body
of the let, and inside the right-hand-sides of the bindings in the case of a
recursive let.

\FIR{} supports let-binding terms, (opaque) types, and datatypes.

The typing rules for let are somewhat complex, but are crucially responsible for
managing the scopes of the bindings defined in the let. In particular:
\begin{itemize}
\item The bindings defined in the let are \emph{not} in scope when checking
  the right-hand sides of the bindings if the let is non-recursive, but
  \emph{are} in scope if it is recursive.
\item The bindings defined in the let are \emph{not} in scope when checking
  the type of the entire binding.\footnote{This is the same device usually employed when
    giving typing rules for existential types to ensure that the inner type does
    not escape.}
\end{itemize}

The behaviour of type-let is also worth explaining. Type-lets are more like
\emph{polymorphism} than type aliases in a language like Haskell. That is, they
are opaque inside the body of the let, whereas a type alias would be
transparent.
This may make them seem like a useless feature, but this is not so.
Term-lets are useful for binding sub-expressions of
term-level computations to reusable names; type-lets are similarly useful for binding
sub-expressions of \emph{type-level} computations.

\section{Compilation}

We will show how to compile \FIR{} by defining a compilation scheme for each
feature in \FIR{}:
\begin{itemize}
\item Non-recursive bindings of terms ($\compileterm$, \cref{sec:non-recursive-terms}) and types ($\compiletype$, \cref{sec:non-recursive-terms})
\item Recursive bindings of terms ($\compiletermrec$, \cref{sec:recursive-terms})
\item Non-recursive bindings of datatypes ($\compiledata$, \cref{sec:non-recursive-data})
\item Recursive bindings of datatypes ($\compiledatarec$, \cref{sec:recursive-data})
\end{itemize}
We do not consider recursive bindings of types, since the case of recursive datatypes is
much more interesting and subsumes it.

Although our goal is to compile to \FOMF{}, since it is a subset of \FIR{} we
can treat each pass as targeting \FIR{}, by eliminating one feature from the language
until we are left with precisely the subset that corresponds to \FOMF{}. This
has the advantage that we can continue to features of \FIR{} until the point in the
pipeline in which they are eliminated.\footnote{An elegant extension of this
  approach would be to define an indexed \emph{family} of languages with
  gradually fewer features. However, this would be a distraction from the main
  point of this paper, so we have not adopted it.}

In particular, we will use non-recursive let-bindings in $\compiletermrec$ and
$\compiledatarec$, which imposes some ordering constraints on our passes.

\subsubsection{Homogeneous let-bindings.}

We have said that we are going to compile e.g. term and type bindings
separately, but our syntax (and typing rules) allows for let terms with many
bindings of both sorts. While this is technically true, it is an easy problem to avoid.

Non-recursive bindings do not interfere with each other, since the
newly-defined variables cannot occur in the right-hand sides of other bindings.
That means that we can always decompose a single term with $n$ bindings into $n$
separate terms, one for each binding. Hence we can consider each sort of binding
(and indeed, each individual binding) in isolation.

The same is not true for recursive bindings. To simplify the presentation we
add a restriction to the programs that we compile: we require recursive
lets to be \emph{homogeneous}, in that they must only contain one sort
of binding (term, type, or datatype). This means that we can similarly consider
each sort of binding in isolation, although we will of course need to consider
\emph{multiple} bindings of the same sort.

This restriction is not too serious in practice. Given a recursive let term with
arbitrary bindings:
\begin{itemize}
  \item Types cannot depend on terms, so there are no dependencies from types or
    datatypes to terms.
  \item We do not support recursive type bindings, so there are no dependencies
    from types or datatypes to types.
\end{itemize}
So we can always pull out the term and type bindings into separate (recursive)
let terms. The situation would be more complicated if we wanted to support
recursive types or dependent types.

\input{NonRecursiveTerms}

\inputlagda{RecursiveTerms}

\input{NonRecursiveDataFormal}

\inputlagda{RecursiveData}
\input{RecursiveDataFormal}

\section{Compiler implementation in Agda}
\label{sec:compiler}

As a supplement to the presentation in this paper, we have written a
formalisation of a \FIR{} compiler in Agda.\footnote{The complete source can be found
  in \href{\sourceUrl}{the Plutus repository}.}
The compiler includes the syntax, the type
system (the syntax is intrinsically typed, so there is no need for a
typechecker), and implementations of several of the passes. In particular, we
have implemented:
\begin{itemize}
  \item Type-level compilation of mutually recursive datatypes into \FOMF{} types.
  \item Term-level compilation of mutually recursive terms into \FOMF{} terms.
\end{itemize}

\noindent The Agda presentation uses an intrinsically-typed syntax, where
terms are identified with their typing derivations \cite{altenkirch.reus:welltyped}. This means that the
compilation process is provably kind- and type-preserving.

However, the implementation is incomplete. The formalization is quite involved
since the term-level parts of datatypes (constructors) must exactly line up with
the type-level parts. Moreover, we have not proved any soundness results beyond
type preservation. The complexity of the encodings makes it very hard to
prove soundness. The artifact contains some further notes on the difficulties in the implementation.

\section{Optimization}

\FIR{} has the virtue that it is significantly easier to optimize than
\FOMF{}. Here are two examples.

\subsection{Dead binding elimination}

Languages with let terms admit a simple form of dead code elimination: any
bindings in let terms which are unused can be removed. A dead binding in a
\FIR{} term can be easily identified by
constructing a dependency graph over the variables in the term, and eliminating
any bindings for unreachable variables.

We can certainly do something with the compiled form of simple, non-recursive let bindings
in \FOMF{}. These are compiled to immediately-applied
lambda abstractions, which is an easy pattern to identify, and it is also
easy to work out whether the bound variable is used.

Recursive let bindings are much trickier. Here the compiled structure is
obscured by the fixpoint combinator and the construction and deconstruction of
the encoded tuple, which makes the pattern much harder to spot. Datatype
bindings are similarly complex.

The upshot is that it is much easier to perform transformations based on the
structure of variable bindings when those bindings are still present in their
original form.

\subsection{Case-of-known-constructor}

The case-of-known-constructor optimization is very important for functional
programming languages with datatypes (see e.g. \cite[section
5]{jones1998transformation}).
When we perform a pattern-match on a term which we know is precisely a constructor
invocation, we can collapse the immediate construction and deconstruction.

For example, we should be able to perform the following transformation:
\begin{align*}
  \Match\ \{\Int\}\ (\Just\ \{\Int\}\ 1)\ 0\ (\lambda x . x+1) \Longrightarrow (\lambda x . x+1)\ 1
\end{align*}
This is easy to implement in \FIR{}, since we still have the
knowledge of which constructors and destructors belong to the same datatype. But
once we have compiled to \FOMF{} we lose this information. A
destructor-constructor pair is just an inner redex of the term, which
happens to reduce nicely. But reducing arbitrary redexes is
risky (since we have no guarantee that it will not grow the program), and we do not know of a general
approach which would identify these redexes as worth reducing.

\section{Why not support these features natively?}

The techniques in this paper cause a significant amount of runtime overhead. The
combinator-based approach to defining recursive functions requires many more
reductions than a direct implementation which could implement recursive calls
by jumping directly to the code pointer for the recursive function.

Similarly, representing datatype values as functions is much less efficient than
representing them as tagged data.

However, there are tradeoffs here for the language designer. If the language is
intended to be a competitive general-purpose programming language like Haskell,
then these performance losses may be unacceptable. On the other hand, if we care
less about performance and more about correctness, then the benefits of having a
minimal, well-studied core may dominate.

Moreover, even if a language has a final target language which
provides these features natively, a naive but higher-assurance backend can
provide a useful alternative code generator to test against.

Of course, the proof is in the pudding, and we have practical experience using
these techniques in the Plutus platform \cite{plutusgithub}. Experience shows
that the overhead proves not to be prohibitive: the compiler is able to compile and run substantial
real-world Haskell programs, and is available for public use at \cite{plutusplayground}.

\section{Related work}

\subsection{Encoding recursive datatypes}

Different approaches to encoding datatypes are compared in \cite{scott}. The
authors provide a schematic formal description of Scott encoding, but ours is
more thorough and includes complete handling of recursive types.

Indexed fixpoints are used in \cite{fixmutualgeneric} to
encode regular and mutually recursive datatypes as fixpoints of pattern
functors. We use the same fixpoint operator \textemdash{} they call it ``hfix'',
while we call it ``ifix''. They also use the trick of encoding products with a tag,
but they use the natural numbers as an index, and they do not handle
parameterized types. Later work in \cite{loh2011generic} does handle
parameterized types, but our technique of putting the parameters into the tag
type appears to be novel. Neither paper handles non-regular datatypes.

There are other implementations of \FOM{} with recursive types.
Brown and Palsberg \cite{Brown2017} use isorecursive types, and includes an indexed fixpoint
operator as well as a typecase operator. However, the index for the fixpoint
must be of kind $\Type$, whereas ours may be of any
kind. Cai et al. \cite{cai-giarrusso-ostermann} differ from this paper both in using equirecursive
types and in that their fixpoint operator only works at kind $\Type$. Moreover,
algebraic datatypes are supported directly, rather than via encoding.

\subsection{Encoding recursive terms}

There is very little existing material on compiling multiple mutually recursive
functions, especially in a strict language. Some literature targets
lower-level or specialized languages
(\cite{hirschowitz2003compilation,syme2006initializing,nordlander2008unrestricted}),
whereas ours is a much more standard calculus. There
are some examples which use fixpoint combinators (such as
\cite{olegfixpoints}, extending \cite{goldberg2005variadic} for typed languages)
which use different fixpoint combinators.

\subsection{Intermediate representations}

GHC Haskell is well-known for using a fairly small lambda-calculus-based IR
(``Core'') for almost all of its intermediary work \cite{jones1998transformation}. \FIR{} is very inspired by
GHC Core, but supports far fewer features and is aimed at eliminating constructs
like datatypes and recursion, whereas they are native features of GHC Core.

A more dependently-typed IR is described in \cite{Henk}. We have
not yet found the need to generalize our base calculus to a dependently-typed
one like Henk, but all the techniques in this paper
should still apply in such a setting. Extensions to Henk that handle let-binding and datatypes are
discussed, but it appears that these are intended as additional native features rather than
being compiled away into a base calculus.

\section{Conclusion}

We have presented \FIR{}, a reusable, typed IR which provides several typical
functional programming language features. We have shown how to compile it into
\FOMF{} via a series of local compilation passes, and given a reference
implementation for the compiler.

There is more work to do on the theory and formalisation of \FIR{}. We
have not given a direct semantics, in terms of reduction rules or otherwise.
We would also like to prove our compilation correct, in that it commutes with
reduction. A presentation of a complete compiler written in Agda with
accompanying proofs would be desirable.

We could also remove some of the restrictions present in this paper: in
particular the lack of mutually recursive type bindings, and the
requirement that recursive let terms be homogeneous.

\paragraph{Acknowledgments.}
The authors would like to thank Mario Alvarez-Picallo and Manuel Chakravarty
for their comments on the manuscript, as well as IOHK for funding the research.

\bibliographystyle{splncs04}
\bibliography{references}

\end{document}
