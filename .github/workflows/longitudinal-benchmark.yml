# This workflow runs the benchmarks defined in the environment variable BENCHMARKS.
# It will collect and aggreate the benchmark output, format it and feed it to the
# github-action-benchmark action.
#
# The benchmark charts are live at https://intersectmbo.github.io/plutus/dev/bench
# The benchmark data is available at https://intersectmbo.github.io/plutus/dev/bench/data.js
#
# This is a performance regression check that is run on every push master.

name: "ðŸ©º Longitudinal Benchmark"

on:
  push:
    branches:
      - master

permissions:
  # Deployments permission to deploy GitHub pages website
  deployments: write
  # Contents permission to update benchmark contents in gh-pages branch
  contents: write

jobs:
  run:
    name: Run
    runs-on: [self-hosted, plutus-benchmark]
    steps:
      - name: Checkout 
        uses: actions/checkout@main 

      - name: Run Benchmarks
        env: 
          BENCHMARKS: "validation validation-decode nofib marlowe"
        run: nix develop --no-warn-dirty --accept-flake-config --command bash ./scripts/run-longitudinal-benchmarks.sh

      # We need this otherwise the next step will fail with:
      # `pre-commit` not found.  Did you forget to activate your virtualenv?
      # This is because github-action-benchmark will call git commit outside nix develop.
      - name: Disable Git Hooks
        run: git config core.hooksPath no-hooks

      - name: Deploy Results
        uses: benchmark-action/github-action-benchmark@v1.20.3
        with:
          name: Plutus Benchmarks
          tool: 'customSmallerIsBetter'
          output-file-path: output.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          # Push and deploy GitHub pages branch automatically
          auto-push: true
          # Enable alert commit comment
          comment-on-alert: true
          # Mention @IntersectMBO/plutus-core in the commit comment so that the 
          # team is notified via GitHub.
          alert-comment-cc-users: '@IntersectMBO/plutus-core'
          # It is a ratio indicating how worse the current benchmark result is. 
          # For example, if we now get 110 ns/iter and previously got 100 ns/iter, it got 10% worse.
          # In this case we alert if it gets 5% worse.
          alert-threshold: '105%'
